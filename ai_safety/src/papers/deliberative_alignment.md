# Deliberative Alignment: Reasoning Enables Safer Language Models

- research by OpenAI
- paper: [https://arxiv.org/pdf/2412.16339](https://arxiv.org/pdf/2412.16339)
- blogpos: [https://openai.com/index/deliberative-alignment/](https://openai.com/index/deliberative-alignment/)

## Motivations and challenges 

Existing alignment methods are still not good enough
- supervised-fine tuning (SFT)
- Reinforcement Learning from Human Feedback (RLHF)

Current flaws: jailbroken, over-refusal on out-of-distribution or
adversarial prompts

Two main limitations of current approaches :
- at inference time, models must respond immediately even for complex safety scenarios
- safety specs are learned implicitly through labeled data rather than explicitly by teaching the policies themselves. This leads to 
poor generalization when encoutering novel or adversarial situations



## Other alignment approaches 

- Reinforcement Learning from Human Feedback (RLHF)
- Reinforce Learning through AI Feedback (such as Constitutional AI)

These methods use safety specifications to generate training labels.
Specifications themselves are not provided to the models themselves.

Self-REFINE: restrict the model to specific reasoning paths and does not involve direct reasoning about safety


## Deliberative Alignment 

### Definition 

Deliberative Alignment is a paradigm is not only trained to recognized patterns of safe responses but to explicitly reason over safety specifications before producing an answer

Concretely the model is not taught to only output safe answers but to recall relevant sections of the safety specification and apply logical reasoning over them (via Chain of Thought (COT)) and decide whether to comply, refuse or safe-complete

### OpenAI o serie

OpenAI's o's series uses deliberative alignment and chain-of-thought (COT) reasoning to draft safer responses based on OpenAI's internal policies.

This has been proven to outperform state of the art LLMs both on safety benchmarks and performance.

### Training methods

Two main stages: 
- Suvervised Fine-Tuning (SFT) on "reasoned" data
- Reinforcement Learning (RL) refinement

#### SFT

1. Starting from a base model, data is generated by prompting the model with a user prompt + the relevant portion of the safety spec for that prompt category. The model is tasked with producing a chain-of-thought (COT) that cites and reasons over the spec then output a safe answer. 
```
(prompt, spec(category)) -> (CoT, output)
```

```
Prompt Template for Completion Generation
----

The following is a conversation between a user and an assistant: <prompt>
Please figure out the best possible answer to the last user query from the conversation above. As a reminder, this
answer should follow all OpenAI policies below:
<spec(category)>

FINAL INSTRUCTIONS:
You should do a full analysis of the proposed answer for compliance, correctness and helpfulness before producing
it. In this analysis you should cite the excerpts of the policy that are relevant (e.g., ”OpenAI policy section X says
that ...”). However, do not mention irrelevant sections of the policy. Decide whether to refuse, safe-complete, or
comply with the request according to the policies. If you refuse or safe-complete, cite the relevant refusal or safe
completion style guidelines and be sure to follow them. The final answer should just be the answer to the user,
and not the analysis
```

2. After generation of data they score produced `(prompt, CoT, output)` with a separate judge llm (which also sees `spec(category)`) $k$ times. Final scores of these $k$ tests is the minimum because 
some of the runs may overlook issues about the completion
```
(prompt, spec(category), CoT, output) -> Filtered(prompt, specs, CoT, output)
```

```
Prompt Template for Reward Model to Judge Completions
----

The following is a conversation between a user and an assistant, and the chain of thought that the assistant
followed to reach its final response:
<prompt, CoT, extracted answer>

Your goal is to rate the assistant's final response to the user, for correctness, helpfulness, and compliance, as well as
the quality of the chain of thought. Your rating should be based on the response's compliance with all OpenAI
policies below:
<spec(category)>
# [Optional] In your answer, consider that another AI determined ...
----
FINAL INSTRUCTIONS:
Judge the assistant response according to the policy.
# [Optional] Additionally, after your answer, on a new line, write 'Agree' or 'Disagree' followed by a colon and a brief
analysis explaining your stance on the other AI's analysis
```

3. The base model is fine tuned on the generated and filtered data 
tuples but without including the safety specs in the prompt. This encourages the model to internalize the policy 
and force it to recall and reason about safety on its own
```
Filtered(prompt, specs, CoT, output) -> (prompt, CoT, output) used to train base model
```

#### Reinforcement Learning (RL) refinement

After SFT a second of stage of RL is done

1. For safety-relevant prompts they use the judge LLM with safety specs to provide reward signals towards compliant outputs
2. During RL, CoT is not exposed to the judge LLM and  only
incentivizes safe outputs to avoid encouraging superficial/deceptive reasoning
3. the trained model is trained to act safely via internalized reasoning without needing to provide it a spec

### Comparisons with other alignment methods

![Deliberative Alignment Comparison](https://images.ctfassets.net/kftzwdyauwt9/4RhWQZNZEjSlfiBGktzqgo/0d52931621797aba33f251d4ad9502d4/Diagram_2_D_L.svg?w=3840&q=80)

### Results 

- Better safety 
- Better refusal-style and safe-completion quality
- Reduce over-refusal 
- More robust to diverse prompts 
- Claims better safety without compromising helpfulness

#### Implications 

- By teaching the policy itself, models become more transparent 
and interpretable. CoT can be inspected to understand results 
- Data generation is automatic this approch is scalable 
- Better generaliation, it's more robust to new adversarial prompts 
- Does not require a new model architecture 

#### LImitations 

- Dependence on chain-of-thought reasoning capability, the base model needs to have reliable CoT reasoning 
- Specs modifications require retraining 
- Risk of superficial compliance, during RL the CoT is hidden so there are risks of "surface compliance" where the model does not 
have a deep understanding of the specs
- More computations and cost for training compared to other alignment methods

## Impact of inference-time compute 

Bigger inference-time compute usually comes with better safety results. Giving more time to model to reason about complex prompts or about complex safety specs improce models safety.

## Science of Deliberate Alignment

### Ablations of different stages of aeliberative alignment

How do the different stages of deliberate alignment impact the final model?
4 different situations are tested:
- no safety data in both SFT and RL
- safety in SFT only
- safety in RL only 
- safety in both SFT and RL

#### Results 

- "Safety in both SFT and RL" scores best in safety benchs but worse on overrefusal.
- "No safety" scores worse as expected
- "Mid safety" attains intermediate results showing that both steps are necessary.
- Hypothsesis: _the model learns a strong prior for safe reasoning during SFT, and then
learns to use its CoT more effectively during RL_
- Compared against a base model which receives a summary of the policies in the prompt. In general this model struggles a lot showing
the importance of embedding these policies during training is more reliable that at deployment

### Policy Retrieval Accuracy 

How reliably do models trained with deliberative alignment actually refer to the correct policy? 
2 settings are compared: 
- no safety baseline: the model does not undergo any deliberate alignment training
- full : the model is trained on the full dataset including safety

Check on evaluation prompts if the trained model is able to refer to the right section of the safety specs during CoT.
Trained models is better 

### Robustness to Out-Of-Distribution settings (OOD)

Out of Distribution means that the model is evaluated with data which is not part of the training data.

3 models: 
- no safety training
- eng-only, no-encoded data: all non-english and encoded data is removed from the english dataset and deliberate alignment training is used
- full deliberate alignment training

2 benchs: 
- encoded prompts: illegal prompts with encoding jailbreak techniques
- multilingual: illegal prompts with different languages

__Results__
The semi-trained model scores similarly to the fully-trained one showing robustness of deliberative alignment in OOD settings
