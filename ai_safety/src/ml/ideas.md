# Ideas 

Ideas I'd like to experiment on. 

## Multiple instances of judge llm 

I believe llm which are too flexible/generic are more prone to be swayed  by jailbreaking techniques.
So as a guardrail, rather than training a GenAI to judge  what content is harmful I'd use multiple small models to judge the safety 
of generated output.
in french "seuls les idiots ne changent pas d'avis" 
I prefer to have inflexible idiots as guardrails models.

## How training 

With deliberative alignment we saw that the more the safety policy 
was ingrained into the model training the safer it became.
This sounds really like human behaviour where teaching
someone why something is "good" and reason about it 
is more robust than just enumerating things that are good or bad.

Experiments that use human education techniques that can be applied to llms:
- what happens when an llm keep b
