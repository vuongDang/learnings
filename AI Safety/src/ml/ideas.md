# Ideas 

Ideas I'd like to experiment on. 

## Multiple instances of judge llm 

I believe llm too big are more prone to be swayed 
by jailbreaking techniques 
So as a guardrail rather than training the GenAI to reason about
what's harmful I'd use multiple small models to judge the safety 
of the generated output
in french "seuls les idiots ne changent pas d'avis" 
I prefer to have inflexible idiots as guardrails models.

## How training 

With deliberative alignment we saw that the more the safety policy 
was ingrained into the model training the safer it became.
This sounds really like human behaviour

Experiments with root values similar to human
