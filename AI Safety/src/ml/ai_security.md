# AI-security

## Categories 

3 main aspects of ai-security
- Model-level safety 
  - ensures that the model behavior is safe and aligned with our needs
- Systems security 
  - how the model interacts with its environment 
- Operational security
  - access, deployment, monitoring
  
### Model-level safety

Does the AI model itself behaves safely?

- Training-time safety: make sure the dataset does not teach harmful patterns
- Robustness: the model does not break or behaves unpredictably when given weird or adversarial inputs
- Alignment: the model's goal match human intent
- Red-teaming/evaluation: testing the model to find failure modes
- Hallucinaiton and bias control: preventing unsafe outputs

### Systems Security

If the model is safe, how do we make sure the overall system around it is safe?

- API hardening: a user cannot escalate privileges or extract the model
- Sandboxing: restring what the model can do 
- Input/output controls: validate user inputs and sanitize model outputs
- Dependence vulnerabilities: support systems aren't exploited (vector DBs, plugins, tools, GPUs)
- Model supply chain security: ensure weights and build aren't tampered with 
- Inference-time attack prevention: protect against prompt injection, data exfiltration or malicious downstream actions

 ### Operational Security

Who gets access? How do we deploy and monitor the system safely?

- Access control: who can use the model? who can modify it?
- Secrets management: protect API keys, credentials and embeddings 
- Deployment security: protect model weights during shipping, loading and inference
- Monitoring & logging: track abnormal behavior, data leakage, misuse attempts
- Incident response: plans for containment when something goes wrong
- Governance & compliance: make sure deployments follow 
internal and external rules

## External Safeguards

External tools that can be used on top of llms to moderate generated output

- Perspective API (from Google Jigsaw)
  -  text moderation api that scores input/output for toxicity, profanity, threat, identity attack...
  - mainly tuned for english and general toxicity
  - not good for specific forbidden topics such as misinformation or illegal advice
  
- OpenAI moderation endpoint 
  - api that scores content for categories like violence, hate, sexual, self-harm...
  - may miss obfuscated unsafe outputs, effectiveness depends on thresholds and model coverage
  
- Guardrails ai 
  - thrid-party open-source framework that wraps llms and provides rule-based safeguards, regex filters and moderation logic

## Tools 

- [Guardrails AI](https://www.guardrailsai.com/docs/getting_started/why_use_guardrails/), opensource framework to easily validate outputs
generated by a llm. Validators are available and shared publicly on [Guardrails Hub](https://hub.guardrailsai.com/)
- [GenBouty](https://genbounty.com), platform that let companies share their ai-dependent products to be tested for security.
  - gives legal targets to attack
  - targets are real products and not only toy examples
  - learn how regulatory framewoks view vulnerability 
  - provides structured feedback
